{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27303d49-8821-4492-921d-07b96d27b09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Email Spam Detection – Assignment (Set 5)\n",
    "\n",
    "**Course**: DSECLZG530  \n",
    "**Dataset**: Email Spam Detection Dataset (classification) – Kaggle  \n",
    "\n",
    "**Group No**: 45  \n",
    "**Member Name(s)**:  \n",
    "- Lakshmi Sahithi Uppu : 2024da04343\n",
    "- Jitendra Kumar : 2024da04067\n",
    "- Jyotirvasu Sharma : 2024da04068\n",
    "- Shiwam Kumar Suman : 2024da04069\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of Part I of the task is to use raw textual data in language models for recommendation based application.\n",
    "\n",
    "The goal of Part II of task is to implement comprehensive preprocessing steps for a given dataset, enhancing the quality and relevance of the textual information. The preprocessed text is then transformed into a feature-rich representation using a chosen vectorization method for further use in the application to perform similarity analysis.\n",
    "\n",
    "\n",
    "**Part I**:  \n",
    "Use all email texts as a training corpus to build a **Trigram probabilistic language model** and compare the following two test sentences. Recommend which sentence is more relevant to the training corpus.\n",
    "\n",
    "- Test Sentence 1: *\"Please review the attached document for our meeting agenda.\"*  \n",
    "- Test Sentence 2: *\"Congratulations! You've won a million dollars, click here now!\"*\n",
    "\n",
    "**Part II**:  \n",
    "Perform sequential tasks:\n",
    "1. Text Preprocessing (tokenization, lowercasing, stopword removal, stemming, lemmatization).  \n",
    "2. Feature Extraction using **TF–IDF word embeddings**.  \n",
    "3. Similarity Analysis: find top two most similar words based on TF–IDF word vectors, justify similarity metric and feature design, and visualize a subset of word embeddings in 2D using **PCA**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faebe70b-f6c7-4e0a-a531-eded2e9103b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Imports\n",
    "# \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# ML / Vectorization / Similarity / PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a605ca5-9398-47d3-a8cc-42ecd2c3cdda",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765725787211}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Load dataset from github repo\n",
    "# Note: The kaggle dataset is being copied from kaggle site and kept in github public repository for easy access.\n",
    "# \n",
    "\n",
    "# Using the raw CSV file link from GitHub\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jyotirvasu/Email_Spam_Detection/main/spam.csv\",\n",
    "    encoding=\"ISO-8859-1\"\n",
    ")\n",
    "print(df.columns)\n",
    "\n",
    "# Use the correct column name for the email text\n",
    "# For this dataset, it is usually 'v2'\n",
    "df = df[[\"v2\"]].rename(\n",
    "    columns={\"v2\": \"emails\"}\n",
    ")\n",
    "df = df.dropna(subset=[\"emails\"])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60483b6-cd49-4283-821b-fce6b9b5f49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# dataset overview\n",
    "# \n",
    "\n",
    "print(\"Number of emails in dataset:\", len(df))\n",
    "print(\"\\nSample rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Show basic length statistics of emails\n",
    "email_lengths = df[\"emails\"].astype(str).apply(len)\n",
    "print(\"\\nEmail length statistics (characters):\")\n",
    "print(email_lengths.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbab7c5-a324-49b2-b26b-b1829ade4fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part I – Trigram Probabilistic Language Model\n",
    "\n",
    "We build a **Trigram language model** from the email corpus.\n",
    "\n",
    "For a sentence \\( w_1, w_2, \\ldots, w_n \\), the probability under a trigram model is approximated as:\n",
    "\n",
    "\\[\n",
    "P(w_1^n) \\approx \\prod_{i=3}^{n} P(w_i \\mid w_{i-2}, w_{i-1})\n",
    "\\]\n",
    "\n",
    "We use **Laplace (add-1) smoothing**:\n",
    "\n",
    "\\[\n",
    "P(w_i \\mid w_{i-2}, w_{i-1}) = \n",
    "\\frac{\\text{count}(w_{i-2}, w_{i-1}, w_i) + 1}\n",
    "{\\text{count}(w_{i-2}, w_{i-1}) + V}\n",
    "\\]\n",
    "\n",
    "where \\( V \\) is the vocabulary size.\n",
    "\n",
    "We then compute **log-probabilities** for the two test sentences and recommend the sentence with higher log-probability as **more relevant** to the training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daca9f52-d1cc-4216-8a27-f1c52ab5b12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Part I: Trigram Language Model\n",
    "# \n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"Lowercase + keep only alphanumeric & space.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_emails\"] = df[\"emails\"].apply(clean_text_basic)\n",
    "\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "trigram_counts = Counter()\n",
    "\n",
    "def tokenize_sent(text):\n",
    "    return text.split()\n",
    "\n",
    "# Build trigram counts from entire corpus\n",
    "for email in df[\"clean_emails\"]:\n",
    "    tokens = tokenize_sent(email)\n",
    "    tokens = [START, START] + tokens + [END]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram_counts[(tokens[i],)] += 1\n",
    "        if i >= 1:\n",
    "            bigram_counts[(tokens[i-1], tokens[i])] += 1\n",
    "        if i >= 2:\n",
    "            trigram_counts[(tokens[i-2], tokens[i-1], tokens[i])] += 1\n",
    "\n",
    "V = len(unigram_counts)  # vocabulary size\n",
    "print(\"Vocabulary size (unigrams):\", V)\n",
    "\n",
    "def trigram_prob(w1, w2, w3):\n",
    "    \"\"\"Laplace-smoothed trigram probability P(w3 | w1, w2).\"\"\"\n",
    "    trig = (w1, w2, w3)\n",
    "    big = (w1, w2)\n",
    "    num = trigram_counts[trig] + 1\n",
    "    denom = bigram_counts[big] + V\n",
    "    return num / denom\n",
    "\n",
    "def sentence_log_prob(sentence):\n",
    "    \"\"\"Return log10 probability of a sentence under trigram LM.\"\"\"\n",
    "    s = clean_text_basic(sentence)\n",
    "    tokens = s.split()\n",
    "    tokens = [START, START] + tokens + [END]\n",
    "    log_p = 0.0\n",
    "    for i in range(2, len(tokens)):\n",
    "        w1, w2, w3 = tokens[i-2], tokens[i-1], tokens[i]\n",
    "        p = trigram_prob(w1, w2, w3)\n",
    "        log_p += math.log10(p)\n",
    "    return log_p\n",
    "\n",
    "# Test sentences\n",
    "test_sentence_1 = \"Please review the attached document for our meeting agenda.\"\n",
    "test_sentence_2 = \"Congratulations! You've won a million dollars, click here now!\"\n",
    "\n",
    "log_p1 = sentence_log_prob(test_sentence_1)\n",
    "log_p2 = sentence_log_prob(test_sentence_2)\n",
    "\n",
    "print(\"Log P(Test Sentence 1):\", log_p1)\n",
    "print(\"Log P(Test Sentence 2):\", log_p2)\n",
    "\n",
    "if log_p1 > log_p2:\n",
    "    print(\"\\nRecommended: Test Sentence 1 is more probable under the trigram model.\")\n",
    "else:\n",
    "    print(\"\\nRecommended: Test Sentence 2 is more probable under the trigram model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5527497-6e6d-4003-af0b-f0400730afc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interpretation (Part I)\n",
    "\n",
    "- We trained a **trigram language model** using all emails as the training corpus.  \n",
    "- We applied **Laplace smoothing** to handle unseen trigrams.  \n",
    "- For each test sentence, we computed the **log-probability** under this model.  \n",
    "- The **sentence with higher log-probability** is recommended as more relevant to the training corpus.  \n",
    "- The printed result above clearly states which test sentence is recommended by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8aff5e-fbee-4ac8-829e-f62454b618aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part II – Text Preprocessing, TF–IDF Features & Similarity Analysis\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Perform **text preprocessing** (tokenization, lowercasing, stopword removal, stemming, lemmatization).  \n",
    "2. Use **TF–IDF** to obtain word embeddings.  \n",
    "3. Compute **similarity between word vectors**, identify top two most similar words, and  \n",
    "4. Visualize a subset of word embeddings in **2D using PCA**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92701981-31ba-4d4c-b675-6523fec1b7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Part II (i): Text Preprocessing (NLTK-free)\n",
    "# \n",
    "\n",
    "import re\n",
    "\n",
    "# Simple English stopword list (enough to demonstrate stopword removal)\n",
    "basic_stopwords = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"if\",\"in\",\"on\",\"for\",\"to\",\"of\",\"is\",\"are\",\"am\",\"was\",\"were\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\"it\",\"as\",\"at\",\"by\",\"with\",\"from\",\"be\",\"have\",\"has\",\"had\",\n",
    "    \"you\",\"your\",\"yours\",\"me\",\"my\",\"we\",\"our\",\"they\",\"their\",\"them\",\"he\",\"she\",\"his\",\"her\"\n",
    "}\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    \"\"\"Tokenization + lowercasing using regex.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "def simple_stem(word: str) -> str:\n",
    "    \"\"\"Very crude stemmer: strips common suffixes for demonstration.\"\"\"\n",
    "    for suf in [\"ing\", \"edly\", \"edly\", \"ed\", \"ly\", \"es\", \"s\"]:\n",
    "        if word.endswith(suf) and len(word) > len(suf) + 2:\n",
    "            return word[:-len(suf)]\n",
    "    return word\n",
    "\n",
    "def simple_lemmatize(word: str) -> str:\n",
    "    \"\"\"Very simple lemmatizer: handles some plural forms and irregulars.\"\"\"\n",
    "    irregular = {\n",
    "        \"mice\": \"mouse\",\n",
    "        \"children\": \"child\",\n",
    "        \"geese\": \"goose\",\n",
    "        \"men\": \"man\",\n",
    "        \"women\": \"woman\",\n",
    "        \"teeth\": \"tooth\",\n",
    "        \"feet\": \"foot\"\n",
    "    }\n",
    "    if word in irregular:\n",
    "        return irregular[word]\n",
    "    if word.endswith(\"ies\") and len(word) > 4:\n",
    "        return word[:-3] + \"y\"\n",
    "    if word.endswith(\"ses\") and len(word) > 4:\n",
    "        return word[:-2]\n",
    "    if word.endswith(\"s\") and len(word) > 3:\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def full_preprocess_tokens(text: str):\n",
    "    \"\"\"Return tokens at each preprocessing stage for demonstration.\"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    no_stop = [t for t in tokens if t not in basic_stopwords]\n",
    "    stemmed = [simple_stem(t) for t in no_stop]\n",
    "    lemmatized = [simple_lemmatize(t) for t in no_stop]\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"no_stop\": no_stop,\n",
    "        \"stemmed\": stemmed,\n",
    "        \"lemmatized\": lemmatized\n",
    "    }\n",
    "\n",
    "def preprocess_for_model(text: str) -> str:\n",
    "    \"\"\"Final preprocessed text used for TF–IDF: tokenize, remove stopwords, lemmatize.\"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in basic_stopwords]\n",
    "    tokens = [simple_lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to whole dataset\n",
    "df[\"processed_text\"] = df[\"emails\"].apply(preprocess_for_model)\n",
    "df[[\"emails\", \"processed_text\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a32573-15a2-45eb-a2b5-2aa8f6276b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demonstration of all preprocessing steps on a sample email\n",
    "sample_text = df['emails'].iloc[0]\n",
    "print(\"Original email:\\n\", sample_text)\n",
    "\n",
    "processed = full_preprocess_tokens(sample_text)\n",
    "print(\"\\nTokens:\", processed['tokens'])\n",
    "print(\"\\nAfter stopword removal:\", processed['no_stop'])\n",
    "print(\"\\nAfter stemming:\", processed['stemmed'])\n",
    "print(\"\\nAfter lemmatization:\", processed['lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb099ef-3af8-4957-8b85-8bbdcf45a60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explanation (Text Preprocessing)\n",
    "\n",
    "- **Tokenization** splits each email into words (tokens), enabling word-level analysis.  \n",
    "- **Lowercasing** ensures that “Free” and “free” are treated as the same word.  \n",
    "- **Stopword removal** removes highly frequent but non-informative words like “the”, “and”.  \n",
    "- **Stemming** crudely reduces words to their root (e.g., *running* → *run*), merging inflected forms.  \n",
    "- **Lemmatization** maps words to their dictionary form (e.g., *better* → *good*), giving cleaner normalized tokens.  \n",
    "\n",
    "These steps **reduce noise**, handle **morphological variation**, and improve the quality of features for TF–IDF and similarity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ee9c9d-4e6f-4ee3-b87d-d340dd8da0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Part II (ii): TF–IDF Feature Extraction\n",
    "# \n",
    "\n",
    "corpus = df[\"processed_text\"].tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"TF–IDF matrix shape (num_docs x num_features):\", tfidf_matrix.shape)\n",
    "print(\"Vocabulary size:\", len(tfidf_vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239858c9-245a-4137-a988-f59327c4e88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explanation (TF–IDF)\n",
    "\n",
    "- **TF–IDF (Term Frequency–Inverse Document Frequency)** gives higher weight to words that:  \n",
    "  - appear frequently in a given document (high TF),  \n",
    "  - but are rare across other documents (high IDF).  \n",
    "- This highlights discriminative words (e.g., “free”, “win”, “click”) which are useful for spam detection.  \n",
    "- Each email is represented as a **high-dimensional sparse vector**, where each dimension corresponds to a word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44720706-88d7-4732-8e8e-cd3c752d8b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Part II (iii): Similarity Analysis – find most similar word pair\n",
    "# \n",
    "\n",
    "# Get vocabulary mapping and index -> word mapping\n",
    "vocab = tfidf_vectorizer.vocabulary_          # word -> column index\n",
    "index_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Word vectors = columns of TF–IDF matrix\n",
    "# Shape: (vocab_size, num_docs)\n",
    "word_tfidf_matrix = tfidf_matrix.T\n",
    "\n",
    "print(\"Word embedding matrix shape (num_words x num_docs):\", word_tfidf_matrix.shape)\n",
    "\n",
    "# To avoid huge computation, limit to top N words\n",
    "max_words_for_similarity = min(800, word_tfidf_matrix.shape[0])\n",
    "sub_word_matrix = word_tfidf_matrix[:max_words_for_similarity, :]\n",
    "\n",
    "print(f\"Using {max_words_for_similarity} words for similarity analysis.\")\n",
    "\n",
    "# Cosine similarity between word vectors\n",
    "sim_matrix = cosine_similarity(sub_word_matrix)\n",
    "\n",
    "max_sim = -1.0\n",
    "best_pair = (None, None)\n",
    "\n",
    "num_words = sim_matrix.shape[0]\n",
    "\n",
    "for i in range(num_words):\n",
    "    for j in range(i + 1, num_words):\n",
    "        sim = sim_matrix[i, j]\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            best_pair = (i, j)\n",
    "\n",
    "word1 = index_to_word[best_pair[0]]\n",
    "word2 = index_to_word[best_pair[1]]\n",
    "\n",
    "print(\"Most similar word pair (within examined subset):\")\n",
    "print(f\"Word 1: {word1}\")\n",
    "print(f\"Word 2: {word2}\")\n",
    "print(\"Cosine similarity:\", max_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461aa967-09f8-41c8-95f6-4da97c38d9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Justification of Similarity Metric & Feature Design\n",
    "\n",
    "- **Feature Design**:  \n",
    "  - We represent each word by its **TF–IDF vector across all documents**.  \n",
    "  - If two words appear in **similar sets of emails with similar TF–IDF weights**, they likely play a similar semantic role (e.g., spam-related words like *free*, *win*, *prize*).  \n",
    "\n",
    "- **Similarity Metric – Cosine Similarity**:  \n",
    "  - Measures the **angle between two vectors**, ignoring magnitude.  \n",
    "  - Works very well with **high-dimensional sparse vectors** like TF–IDF.  \n",
    "  - Standard metric in information retrieval and text similarity tasks.  \n",
    "  - A high cosine similarity indicates that two words tend to co-occur in similar documents and hence are semantically related in this corpus.  \n",
    "\n",
    "The printed pair above corresponds to the two words that are most similar according to this representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e96028e-4c77-4975-b39a-2115b3fe58a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# PCA-based 2D Visualization of Word Embeddings\n",
    "# \n",
    "\n",
    "# Choose a small subset of words for visualization\n",
    "max_words_to_plot = min(50, word_tfidf_matrix.shape[0])\n",
    "vis_sub_matrix = word_tfidf_matrix[:max_words_to_plot, :].toarray()\n",
    "vis_words = [index_to_word[i] for i in range(max_words_to_plot)]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(vis_sub_matrix)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "for i, w in enumerate(vis_words):\n",
    "    plt.text(word_vectors_2d[i, 0] + 0.01,\n",
    "             word_vectors_2d[i, 1] + 0.01,\n",
    "             w, fontsize=9)\n",
    "\n",
    "plt.title(\"2D PCA Projection of Word TF–IDF Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f34d92d-c477-4472-bf78-7e1fe02a0484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interpretation of PCA-based 2D Visualization\n",
    "\n",
    "- **PCA (Principal Component Analysis)** reduces the high-dimensional TF–IDF word embeddings to **2 dimensions** while preserving as much variance as possible.  \n",
    "- In the 2D plot:  \n",
    "  - Words that appear in **similar contexts** (e.g., spam-related tokens) tend to cluster closer together.  \n",
    "  - Words used in normal (ham) emails often form a different region.  \n",
    "- This gives a **semantic map of words** in the email corpus and visually supports our similarity analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f76b11-880d-46a6-a681-7de749b719c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. **Built a trigram probabilistic language model** on the entire email corpus and used it to compare two test sentences, recommending the one with higher log-probability as more relevant to the dataset.\n",
    "2. Performed **complete text preprocessing** (tokenization, lowercasing, stopword removal, stemming, and lemmatization) with clear demonstration on a sample email.\n",
    "3. Extracted **TF–IDF features** from the preprocessed emails to obtain a high-dimensional sparse representation suitable for text mining.\n",
    "4. Conducted a **similarity analysis between word embeddings** (based on TF–IDF and cosine similarity) and identified the most similar pair of words in the vocabulary subset.\n",
    "5. Used **PCA to project word embeddings to 2D**, visualizing the semantic structure where words with similar usage tend to cluster together.\n",
    "\n",
    "These steps together shows language modelling, text preprocessing, feature engineering, similarity analysis and visualization on the email spam detection dataset.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Group45",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
